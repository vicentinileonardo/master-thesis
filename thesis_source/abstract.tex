\chapter*{Abstract} % no number
\label{abtract}

\addcontentsline{toc}{chapter}{Abstract} % add to index

%contesto 
%motivazioni
%riassunto problema affrontato
%tecniche utilizzate
%analisi requisiti, 
%analisi pprogetti/prodotti disponibili
%creazione proof of concept
%risultati raggiunti:
%full e2e testing
%contributo personale

%\section{Context}
In the last 20 years, cloud computing has steadily become the backbone of modern digital infrastructure, enabling scalable and flexible deployment of heterogeneous services across geographically distributed data centers around the world.
It can be safely said that cloud computing can be labelled as a disruptive technology since it has revolutionized the way IT services and applications are designed, developed, delivered and consumed.
Moreover, cloud computing is playing a crucial role in the current AI revolution, enabling the training of large-scale machine learning models (such as Large Language Models) in a distributed manner and facilitating the deployment of AI-based services (such as chatbots, AI-powered search engines, etc.).
The central role of cloud computing is also testified on the economic and financial side: indeed, according to recent market research for the year 2024 the total revenues of public cloud infrastructure services reached \$330 billions \cite{statista_cloud_market_share}.
However, the rapid adoption and growth of cloud computing has also raised concerns about its environmental impact.
The reason is that the big computing infrastructures require a huge amount of electricity to operate.
By consequence, this energy consumption is responsible for an important amount of carbon emissions.
Recent efforts in computational sustainability have been made and among these the GreenOps operational paradigm arose.
It must be also said that majors public cloud providers (such as Amazon Web Services, Microsoft Azure, Google Cloud Platform) are already taking steps to reduce their carbon footprint in a variety of ways. 
These spans from internal optimization of data centers, to bringing awareness to the customers on their individual carbon footprint. \newline

The work described in this thesis is part of a larger project which has the primary goal of developing a system for \textbf{reducing the carbon footprint of workloads in the cloud}.
Said system is mainly based on the idea of exploiting \textbf{time-shifting} and \textbf{geographical shifting} of cloud workloads to time periods and geographical regions with low carbon intensity.
As a matter of fact, nowadays, the carbon intensity of electricity varies significantly depending on the time of day and the geographical region.
Therefore, it is deemed interesting to leverage these fluctuations in electricity grid carbon intensity to schedule workloads in a way that reduces the carbon footprint of the workloads compared to a traditional scheduling approach.
Currently, targeted workloads are the ones that are not time-sensitive but instead are quite delay-tolerant.
In addition, workloads are also characterized by other requirements, such as Quality of Service (QoS) requirements (e.g., deadlines, maximum latency constraints, etc.) and cost constraints (e.g., budget constraints, cost optimizations, etc.).
Indeed a \textbf{policy-driven approach} is used to encode these requirements along with scheduling outcomes.
The system is designed to be used in a multi-cloud setting, where workloads can be scheduled on different cloud providers.
Adopting a multi-cloud paradigm offers several advantages, including user-centric flexibility, avoidance of vendor lock-in, and the ability to exploit multiple regions because cloud providers may have varying geographical coverage.
Some of the guiding principles adopted in the design of the system are: flexibility, extensibility, cloud-agnosticism.
In order to implement these principles, the use of \textbf{Kubernetes as a platform for multi-cloud resource management} is considered. 

The project is carried out in collaboration with Krateo SRL, a company that is developing a modular platform for resource management and with Electricity Maps, a company that provides historic and real-time data on the carbon intensity of electricity grids around the world. 
The former provided support for platform integration and supplied the infrastructure used for testing while the latter was the data provider.
\newpage

%The goal of the project is to employ mainly time-shifting and geographical shifting for the scheduling of workload leveraging a multi-cloud setting.
%We can choose to schedule workloads in periods and regions with low carbon intensity (when renewables are plentiful). 
%Therefore, targeted workloads are the ones that are not time-sensitive but instead are quite delay-tolerant. 
%For example training a machine learning model could wait until a period of low carbon intensity. Another example is shifting video / image processing, as Google is doing.
%Kubernetes is leveraged as a platform for scheduling and managing workloads on different cloud providers.
%Long term goal: “Using electricity when the carbon intensity is low is the best way to ensure investment flows towards low-carbon emitting plants and away from high-carbon emitting plants”.

%\section{Problem statement}

This thesis addresses the following research question: ``How can flexible and dynamic resource management be achieved to satisfy a varied set of requirements in the context of multi-cloud environments?''

To tackle this wide and complex problem, this research envisions, designs, and implements a policy-driven architecture based on the Kubernetes ecosystem.
In particular, we propose and describe a multi-cloud resource management system that is mainly built upon Kubernetes, Krateo PlatformOps and Open Policy Agent (OPA).

Some of the objectives that the system aims to achieve are the following: multi-cloud compatibility, policy-driven resource management, extensibility and flexibility.
Multi-cloud compatibility is achieved by providing a vendor-agnostic solution that can operate across multiple cloud providers.
Enabling the flexible definition of policies that encode requirements and constraints of workloads (e.g., Service Level Agreements, Quality of Service, economic cost reduction, carbon footprint reduction) is how policy-driven resource management is accomplished.
Extensibility and flexibility are obtained by designing a modular system that can be adapted for any kind of cloud resource and arbitrary requirements. 

In particular, the use case taken into consideration in the context of this thesis is the scheduling of virtual machines (VMs) in a multi-cloud environment to minimize their carbon footprint while maintaining an exemplificative set of requirements and constraints.
As a matter of fact, the traditional management of resources and workloads in the cloud does not usually consider the environmental impact, leading to suboptimal energy usage and excessive carbon emissions.
However, users and organizations, even if interested in reducing carbon emissions and energy consumption, may not have the necessary systems to do so.
In addition, they may be reluctant to give up performance and cost requirements in favor of environmental sustainability.
Therefore, the proposed system enables VM scheduling in a way that minimize carbon footprint while satisfying other requirements and constraints. \newline


%Use cases (basic ones for the beginning) higher level explanation here
%first use case ("GreenOps" VM scheduling)

%second: scaling down a vm 
%nfrastructure already put in place

%the system was designed with flexibility in mind therefore a workload could be potentially anything
%the condition is just to be represented in some way and have something else do certain actions based on that representation
%As we will see in section XXX, the most simple of this would be K8s operators
%this is described in section XYZ

%\section{Method}

The research methodology follows a \textbf{hands-on approach} to developing a practical, production-ready solution, integrating and leveraging existing open-source technologies and tools.
The phases of the methodology followed for this thesis are the following:
\begin{enumerate}
    \item \textbf{Literature and Technology Review}: The first phase of the research methodology is to review the existing literature and technologies related to the problem statement. 
    This includes exploring the state-of-the-art of multi-cloud resource management and carbon-aware resource management.
    A study on public cloud providers and the multi-cloud paradigm is also conducted.
    In addition, the review includes identification and study of the existing open-source tools and technologies that can be used to develop the solution (e.g., Kubernetes, Krateo PlatformOps, Open Policy Agent).
    \item \textbf{System Architecture Design}: The second phase of the methodology is to design the system architecture, including the components, interactions, and data flows.
    This includes also the development of various Proof-of-Concepts in order to test the feasibility of the various identified candidate solutions and to understand tools' strengths, limitations, trade-offs.
    \item \textbf{System Implementation}: The implementation phase involves the integration of the system components, the development of policies and policy bundles pipelines, the configuration of cloud resource templates, the set up of the MLOps infrastructure.
    \item \textbf{System Deployment and End-to-End Testing}: The final phase of the methodology is to deploy the system on a real infrastructure and perform end-to-end testing to validate the system's functionality in a real-world multi-cloud scenario. \newline
\end{enumerate}



%---
%The method adopted in this thesis followed several principles described in this section.
%First of all the main goal was to develop a solution to be integrated to be integrated into an existing platform \textbf{to support real use cases}.

%Developing a real solution, integrating it on top of OSS

%production-ready solution

%we are on the consumer side, not on the provider side

%System architecture to start with: Saima's + Krateo platform

%integration into an existing platform (krateo)

%leveraging krateo componenets is beneficial 

%Krateo Core Provider and cdc instead of developing 1 or more K8s operators from scratch


%analysis of possible solutions
%implemnted poc 

%Initial analysis of a solution with operators were tried

%A PoC comprising 1 operator was created 
%``Synchronization operation"
%cons: maintainer costs

%ideation and creation of architectural diagrams

%tackling first use case: GreenOps scheeduling of virtual machines
%but create a system that is flexible and extendible enough to be used for other use cases as well

%\section{Personal contribution}

This thesis represents a technical and research contribution within a larger project supervised by Prof. Sandro Luigi Fiore. 
The project is divided into three main parts: data analysis, machine learning, and cloud infrastructure.
The contribution of this thesis is on the cloud infrastructure part and includes:
\begin{itemize}
    \item The design of the system architecture, including the identification of components, interactions, and data flows.
    \item The study and development of a policy-driven approach to workload scheduling that also enables the encoding of requirements and constraints.
    \item The implementation of the system, including the integration of the system components, the development of policies and policy bundles pipelines, the configuration of cloud resource templates, the set up of the MLOps infrastructure.
    \item The deployment of the system on a real infrastructure and the performance of end-to-end testing to validate the system's functionality in a production-like multi-cloud scenario.
\end{itemize}

For what concerns the integration with the other parts of the project, the system developed in this thesis is designed to be integrated with the machine learning part of the project which can be summarized as a series of machine learning models that predict the carbon intensity of electricity grids and a scheduler that uses these predictions to schedule workloads. 
In order to carry out the end-to-end testing, this scientific work is deployed on the infrastructure leveraging tools described in section \ref{sec:mlops_infrastructure} and referred as MLOps infrastructure.

\newpage
