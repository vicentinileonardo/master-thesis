\chapter{System design and implementation}
\label{cha:design}

This chapter presents the design and implementation of our system, focusing on the integration of the various components and the overall architecture. 
A general description of some of the key components is provided to better explain their role in the system and the reasons for their inclusion.
The system is designed to be modular, scalable, and extensible, enabling the integration of additional components as needed.
We present a Proof of Concept (PoC) implementation of the proposed architecture.

\section{Assumptions}

In this section we present the assumptions made during the design and implementation of the system. 

\subsection{Workload definition}
\label{sec:workload_definition}
In this work, workloads have been modeled as \textbf{Virtual Machines (VMs)}, representing the \textbf{primary use case} considered during the system's initial design phase. 
It is possible to define as ``interrupible workloads" those workloads that can be stopped and restarted without losing the work done. 
For this work, only ``\textbf{non-interruptible workloads}" are considered.
This choice was driven by the fact that VMs are both a common and widely used cloud resource and one of the simplest to provision on multiple cloud providers and therefore the most important aspect to optimize.
For the purpose of this work, we propose the following formalization where a VM is  defined as a tuple:

\[
\text{VM} = (\text{MinCPU}, \text{MinRAM}, D, DL, ML)
\]

with:
\begin{itemize}[itemsep=0.2pt, topsep=1pt]
    \item \( \text{MinCPU} \) is the minimum number of virtual CPUs required.
    \item \( \text{MinRAM} \) is the minimum RAM required (in GB).
    \item \( D \) is the duration for which the VM must run to complete its processing task \( P \) (in hours).
    \item \( DL \) is the deadline timestamp by which the VM must complete execution.
    \item \( ML \) is the maximum allowed latency in milliseconds. If latency is not a constraint, then it can be omitted. \\
\end{itemize}

This VM can be scheduled on any public cloud provider since we are interested in a multi-cloud system where the cloud can be effectively seen as a commodity. 
Alternatively, a subset of \textbf{eligible public cloud providers} can be set at runtime by the user.
We will refer to \textbf{general-purpose VMs} and not specialized ones like GPU instances or high-performance computing instances. \\

As an example, we can define a VM with the following specifications:

\[
\text{VM}_{\text{example}} = (4, 4, 2, \texttt{"2025-03-20T23:59:59Z"}, 100)
\]


where:
\begin{itemize}[itemsep=0.2pt, topsep=1pt]
    \item \( \text{MinCPU} = 4 \) vCPUs
    \item \( \text{MinRAM} = 4 \) GB
    \item \( D = 1 \) hour
    \item \( DL = \texttt{"2025-03-20T23:59:59Z"} \) (i.e., the processing task \( P \) inside the VM must complete before this timestamp)
    \item \( ML = 100 \) ms (i.e., the maximum allowed latency) \\
\end{itemize}

The system is designed to be cloud-agnostic, however for the purpose of this work, the system is currently configured to support three major cloud providers: \textbf{Microsoft Azure}, \textbf{Google Cloud Platform}, and \textbf{Amazon Web Services}.

\subsection{System limitations}

A limitation of our general approach is that only resources supported by the cloud provider’s Kubernetes operator can be provisioned in a 
seamless way.
Not all cloud resources available in a provider’s portfolio are guaranteed to have corresponding Kubernetes CRs. 
This introduces certain constraints:

\begin{itemize}[itemsep=0.2pt, topsep=1pt]
  \item[$\bullet$] Limited resource availability: if a specific resource type is not supported by the cloud provider operator, it cannot be provisioned using the current system.
  \item[$\bullet$] Dependence on operator updates: cloud providers may extend or modify the set of resources supported by their Kubernetes operators over time.
  \item[$\bullet$] Vendor-specific implementations: for the same class of resources (e.g., virtual machines), the structure and fields of the CRs may vary a lot between cloud providers.
\end{itemize}

Despite these constraints, the system architecture remains highly adaptable, and future enhancements could incorporate additional or alternative provisioning mechanisms. 
An example of an alternative implementation could be the \textbf{direct API interactions} with cloud providers to bypass operator limitations. As a matter of fact, usually cloud provider operators are leveraging these APIs under the hood to interact with the cloud provider’s services.
Another approach could involve the development of custom operators or controllers to manage specific resource types not supported by existing operators.
For instance, Krateo PlatformOps provides the so-called ``oasgen-provider'' that aims to fill the gaps of missing or incomplete Kubernetes operators. This module is a Kubernetes controller that is able to generate Kubernetes CRDs and the related controllers from OpenAPI specifications \cite{krateo_oasgen_provider}. 

\section{System Architecture}
\label{sec:system_architecture}

Table \ref{tab:system_components} provides an overview of the main components of the system and their respective functions and Figure \ref{fig:architecture} illustrates the general architecture of the system.

\begin{table}[t]
  \centering
  \renewcommand{\arraystretch}{1.3} % Adjust row height for readability
  \begin{tabularx}{\textwidth}{| l | X |} % 'X' makes the second column auto-wrap
    \hline
    \textbf{Component} & \textbf{Function} \\
    \hline
    Krateo PlatformOps & Provides an abstraction layer for infrastructure orchestration, enabling declarative resource management and integration with cloud providers with templates. \\
    \hline
    Cloud Providers Kubernetes Operators & Manages the provisioning and reconciliation of cloud resources within Kubernetes, ensuring the actual state matches the desired state. \\
    \hline
    Kubernetes Mutating Webhook Configuration & Intercepts and modifies API requests before they are persisted, allowing dynamic configuration adjustments with policy enforcement. \\
    \hline
    OPA Server & Evaluates policy decisions based on defined constraints and input data from Kubernetes API requests through the webhook configuration. \\
    \hline
    OPA Policies and Data & Define the rules and contextual information used by OPA to make policy decisions, namely scheduling information \\
    \hline
    GreenOps Scheduler & Determines the optimal scheduling region and scheduling time for VMs, acting as an external data source for OPA policies. \\
    \hline
    MLflow & Allows the tracking, logging, versioning and storing of machine learning experiments for reproducibility and model lifecycle management. \\
    \hline
    KServe & Provides scalable and Kubernetes-native model serving capabilities, enabling deployment of machine learning models for inference. \\
    \hline
  \end{tabularx}
  \caption{Main components of the system and their respective functions.}
  \label{tab:system_components}
\end{table}

\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{images/architecture.png}
  \caption{General architecture of the system}
  \label{fig:architecture}
\end{figure}

All the components listed in table \ref{tab:system_components} must be deployed inside a Kubernetes cluster. The only exception are the OPA Policies and data which lies outside the cluster as described in section \ref{sec:opa_bundles}.

\section{Krateo PlatformOps integration}
\label{sec:krateo_integration}

Krateo PlatformOps is leveraged in this system as a core component for \textbf{multi-cloud resource management}, as it allows declarative orchestration of cloud resources across different cloud providers leveraging Kubernetes as a control plane. 
This section highlights the differences between two different approaches that were considered for multi-cloud resource management:
\begin{itemize}[itemsep=0.2pt, topsep=1pt]
  \item[$\bullet$] The \textbf{custom Kubernetes ``Synchronization Operator"} approach (initially considered)
  \item[$\bullet$] The \textbf{Krateo PlatformOps} approach (adopted in the final system design) \\
\end{itemize}

It is deemed interesting to describe both approaches in order to identify the several \textbf{trade-offs} between implementing a custom Kubernetes ``Synchronization Operator" and leveraging a template-based abstraction for cloud resource provisioning.

\subsection{Resource management: the custom Kubernetes ``Synchronization Operator" approach}

When dealing with multi-cloud resource management with Kubernetes as a control plane, a \textbf{synchronization and mapping mechanism} (i.e., broker) is required to bridge the gap between:
\begin{itemize}[itemsep=0.2pt, topsep=1pt]
  \item[$\bullet$] \textbf{Generic Kubernetes Custom Resources}, which represent generic provider-agnostic workloads.
  \item[$\bullet$] \textbf{Cloud provider-specific Custom Resources}, which correspond to the actual cloud resources provisioned through the respective Kubernetes operators provided by Azure, AWS or GCP (in our case).
\end{itemize}
This requirement was also highlighted in the literature review described in section \ref{sec:multi_cloud_resource_management_outcomes}.

In this first approach, a custom Kubernetes ``Synchronization Operator" would be responsible for the mapping and synchronization of the above resource types. 
Figure \ref{fig:k8s_operator} illustrates the high-level architecture of the system with the custom operator approach.
This approach is based on the principle of \textbf{continuous reconciliation}, where the operator continuously monitors and adjusts the system to maintain consistency between the desired and actual states.
Candidate solutions for the implementation of a Kubernetes operator includes: Operator SDK, Kubebuilder, or writing the operator from scratch using the Kubernetes client libraries. 
For the purpose of this work, Kubebuilder was used to develop a Proof of Concept (PoC) for the custom operator.

\subsubsection{Responsibilities of the ``Synchronization Operator"}

In our specific case, the operator should continuously watch the generic CRs in the Kubernetes cluster to check if critical scheduling fields have been set, in particular:
\begin{itemize}[itemsep=0.2pt, topsep=1pt]
  \item[$\bullet$] \textbf{scheduling region}: Defines where the workload should be placed.
  \item[$\bullet$] \textbf{scheduling time}: Specifies when the workload should be deployed.
\end{itemize}

These fields, if set, indicate a geographical placement and timing for the workload that have been determined by the GreenOps Scheduler.
If these fields are not yet present, the operator must wait for scheduling decisions before proceeding.
Therefore, inside its reconcile loop, the operator should:
\begin{enumerate}[itemsep=0.2pt, topsep=1pt]
  \item Continuously check if scheduling fields (schedulingRegion, schedulingTime) are set.
  \item Trigger the creation of the provider-specific resource when the schedulingTime is approaching.
  \item Track the provisioning status by marking the generic CR with a field indicating that the cloud-specific resource has been created.
\end{enumerate}

\subsubsection{Post-Creation Considerations}

Once the cloud provider-specific resource is created, two main questions arise:
\begin{itemize}[itemsep=0.2pt, topsep=1pt]
  \item[$\bullet$] What happens if the provider-specific CR is modified manually?
  \item[$\bullet$] What happens if the provider-specific configuration is modified directly on the cloud provider (outside Kubernetes)?
\end{itemize}

Related to the first question, an example scenario could be: changing the VM instance type (VM size) inside the Kubernetes cluster. 
In this case, the operator needs to decide whether to revert unauthorized changes or allow them and update the generic CR accordingly.
For the second question, an example could be: changing the VM size directly on the cloud provider’s console. 
In this case, the operator should detect the drift and update the generic CR to reflect the external changes.
It must be said that this approach conflicts with the ``single source of truth'' paradigm, which is represented by the generic CR in this case.

\subsubsection{Resource linking}

A mechanism must be in place to link the generic CR to the cloud provider-specific CR. 
Possible approaches include:
\begin{itemize}[itemsep=0.2pt, topsep=1pt]
  \item[$\bullet$] UUID-based linking: A universally unique identifier ensuring each resource is mapped correctly.
  \item[$\bullet$] Kubernetes Object Metadata (ObjectMetadata.Name \& ObjectMetadata.Namespace): This approach may be preferable within a single Kubernetes cluster, avoiding the need for an external ID system.
\end{itemize}

\subsubsection{Termination Logic}

The operator must handle the deletion of cloud resources correctly in a variety of scenarios, including:
\begin{itemize}[itemsep=0.2pt, topsep=1pt]
  \item[$\bullet$] When the provider-specific CR is deleted from Kubernetes, the corresponding cloud resource is de-provisioned and the custom operator should ensure the deletion process is handled gracefully, avoiding orphaned generic CRs.
  \item[$\bullet$] If the provider-specific resource is deleted directly on the cloud provider (e.g., on the provider console), the operator should detect the change and update the generic CR accordingly.
  \item[$\bullet$] In the event of a generic CR deletion, the custom operator should ensure the provider specific resource is removed, triggering a deletion process on the cloud provider side (de-provisioning).
\end{itemize}

\subsubsection{Managing cloud provider-specific fields}

Each cloud provider has unique resource configurations and constraints that must be managed.
Some differences are purely syntactic (e.g., AWS uses \textit{instanceType}, whereas Azure uses \textit{vmSize}).
Others require additional provider-specific metadata (e.g., Azure requires a \textit{resourceGroup} field which represent a logical container for resources in Azure).
A custom operator must take into account and encode this logic explicitly, making it more complex to maintain especially when supporting several cloud providers.

\subsubsection{Limitations of a custom ``Synchronization Operator''}
\label{sec:limitations_custom_operator}

Another major challenge with a custom ``synchronization operator'' is that some cloud providers do not support time scheduling metadata within their Custom Resources. 
In particular, no cloud provider operator among the ones we used for the system (AWS, Azure, GCP) currently provides a dedicated field for the scheduling time.
This means that the custom Kubernetes operator itself must handle a time scheduling logic, delaying CR creation until the scheduled time.
If the operator, upon the creation of a generic CR, immediately creates the cloud-provider specific CR (without a ``waiting logic''), the cloud provider operator will trigger and provision the VM immediately, ignoring scheduling constraints.
Due to these limitations and complexities, we explored and leveraged an \textbf{alternative template-driven approach} using Krateo PlatformOps, described in the next section.

\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{images/k8s_operator.png}
  \caption{Multi-cloud resource management with Custom Kubernetes ``Synchronization Operator'' approach}
  \label{fig:k8s_operator}
\end{figure}

\subsection{Resource management: the Krateo PlatformOps approach}

\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{images/krateo.png}
  \caption{Multi-cloud resource management with Krateo PlatformOps approach}
  \label{fig:krateo}
\end{figure}

In our final approach, we opted to logically replace a custom Kubernetes operator (``Synchronization Operator''), originally designed to handle the \textbf{mapping} from generic to cloud-specific resources, with \textbf{Krateo Core Provider}. 
This decision was motivated by the need for \textbf{greater flexibility and maintainability} in defining multi-cloud infrastructure components. 
As a matter of fact, a custom Kubernetes operator was originally designed to handle only virtual machines (VMs).
Mappings and extensions to support additional cloud resources would have required significant code changes and maintenance overhead for each additional resource type added.
Therefore, instead of embedding business logic directly within a custom Kubernetes operator, in the current system implementation, we leverage the capabilities of \textbf{Helm templating} to dynamically generate cloud-provider-specific resources. 
More precisely, another Krateo component, the \textbf{Krateo composition-dynamic-controller} is leveraging \textbf{Helm Template Engine} under the hood to generate Kubernetes resources starting from Helm templates.
By adopting an Helm-based resource generation, we can reduce the maintenance overhead and simplify the system architecture.
Figure \ref{fig:krateo} illustrates, in a high-level manner, the revised system architecture for resource management with Krateo PlatformOps.

This approach, further described in this section, offers several advantages:
\begin{itemize}[itemsep=0.2pt, topsep=1pt]
  \item[$\bullet$] Simplified resource management: Helm enables a standardized way to define resources without developing complex operator logic.
  \item[$\bullet$] Greater extensibility: By externalizing the logic a the custom operator (effectively removing it), future modifications and integrations on the system with additional cloud providers become easier.
  \item[$\bullet$] Reduced maintenance overhead: Custom operators typically require constant updates and refinements, especially if they are responsible for complex business logic. 
\end{itemize}

\subsubsection{Generic VM resource definition}

%n pratica, la prima fase è la creazione del composition-dynamic-controller (cdc), fatta dal core-provider
%Il core-provider non ha bisogno dei valori nella composition resource ma solo delle Custom Resource Definition, poi il cdc crea la composition facendo il templating
%Nel caso in cui si aggiornasse la composition resource con valori aggiuntivi (dal mutating webhook per esempio), sarà rifatto il templating e farà l'Helm upgrade

In order to define a generic VM resource, we leverage Krateo Core Provider to define a \textbf{CompositionDefinition} resource that specifies the structure of the VM resource.
Effectively a CompositionDefinition is a Kubernetes Custom Resource that defines the structure of a Composition Custom Resource (instance) which is an Helm chart comprised of Helm templates and values.
As a matter of fact, a CompositionDefinition is referencing a versioned Helm chart that is stored in a Helm repository (e.g. on a Helm repository server).

\begin{lstlisting}[language=yaml, caption={CompositionDefinition used in the system}, label={lst:composition_definition}]
apiVersion: core.krateo.io/v1alpha1
kind: CompositionDefinition
metadata:
  name: vmtemplate
spec:
  chart:
    repo: vm-template
    url: https://leonardovicentini.com/helm-charts/charts
    version: 1.2.0
\end{lstlisting}

The Helm chart referenced in the CompositionDefinition contains the Helm templates and values needed to define the resource named \textit{VmTemplate}.
Listing \ref{lst:values.yaml} shows an example of the VmTemplate values.yaml file, which defines the fields of the VM resource while listing \ref{lst:values.schema.json} shows the corresponding JSON schema for the \textit{values.yaml} file.
The fields in the \textit{values.yaml} file are used to define the VM resource, including the VM name, CPU, memory, scheduling time, scheduling location, duration, deadline, and maximum latency and include the ones used in the formal definition of a VM resource in section \ref{sec:workload_definition}.

\begin{lstlisting}[language=yaml_1, caption={values.yaml file used in the system}, label={lst:values.yaml}, float=htpb]
# @param {string} vmName Name of the VM
vmName: test-vm

# @param {integer} cpu  Number of CPU cores
cpu: 1

# @param {integer} memory Number of GB of RAM
memory: 2

# @param {string} [schedulingTime] Scheduling Time for the VM
schedulingTime: 2025-05-05T00:00:00Z

# @param {string} [schedulingLocation] Scheduling Location for the VM
schedulingLocation: italynorth

# @param {string} duration Duration of the Workload
duration: 3h

# @param {string} deadline Deadline of the Workload
deadline: 2025-12-31T10:00:00Z

# @param {integer} maxLatency Maximum Latency of the Workload
maxLatency: 100
\end{lstlisting}

\newpage

\lstset{style=jsonstyle}
\begin{lstlisting}[caption={values.schema.json file used in the system}, label={lst:values.schema.json}]
{
  "type": "object",
  "$schema": "http://json-schema.org/draft-07/schema",
  "required": [
    "vmName",
    "cpu",
    "memory",
    "duration",
    "deadline",
    "maxLatency"
  ],
  "properties": {
    "vmName": {
      "type": [
        "string"
      ],
      "description": "Name of the VM",
      "default": "test-vm"
    },
    "cpu": {
      "type": [
        "integer"
      ],
      "description": "Number of CPU cores",
      "default": "1"
    },
    "memory": {
      "type": [
        "integer"
      ],
      "description": "Number of GB of RAM",
      "default": "2"
    },
    "schedulingTime": {
      "type": [
        "string"
      ],
      "description": "Scheduling Time for the VM",
      "default": "2025-05-05T00:00:00Z"
    },
    "schedulingLocation": {
      "type": [
        "string"
      ],
      "description": "Scheduling Location for the VM",
      "default": "italynorth"
    },
    "duration": {
      "type": [
        "string"
      ],
      "description": "Duration of the Workload",
      "default": "3h"
    },
    "deadline": {
      "type": [
        "string"
      ],
      "description": "Deadline of the Workload",
      "default": "2025-12-31T10:00:00Z"
    },
    "maxLatency": {
      "type": [
        "integer"
      ],
      "description": "Maximum Latency of the Workload",
      "default": "100"
    }
  }
}
  
\end{lstlisting}

\subsubsection{Generic VM to Cloud Provider Specific VM mapping}

The Krateo composition-dynamic-controller (CDC) is responsible for creating the Composition CR by templating the Helm chart referenced in the CompositionDefinition.
We leveraged Helm templating to dynamically generate cloud-provider-specific resources from a generic VM resource.
As a matter of fact, the Helm chart contains all the \textbf{three cloud provider specific sets of templates} necessary for a VM provisioning but \textbf{only one set of templates is used at a time}.
The CDC will generate the cloud-provider-specific VM resources based on the cloud provider specified in the generic VM resource.
Therefore, for each generic VM resource, the CDC will use \textbf{only one set of templates} to generate the cloud provider specific VM resource.
This ``brokering mechanism'' (routing through Helm templates) is implemented in the Helm templates using ``\textbf{guards}'' as reported in listing \ref{lst:guards}.
The manifest will be created only if the cloud provider specified in the generic VM resource is the same as the cloud provider specified in the generic VM resource.
This flexibility allows the system to be cloud-agnostic and to support multiple cloud providers as soon as the cloud provider specific templates are available in the Helm chart.
The directory structure of the Helm chart is the following:

\vspace{0.5cm}

\dirtree{%
.1 chart/.
.2 [several utilities files for mappings (omitted)].
.2 templates/.
.3 aws/.
.4 instance.yaml.
.4 subnet.yaml.
.4 vpc.yaml.
.3 azure/.
.4 networkinterface.yaml.
.4 virtualmachine.yaml.
.4 virtualnetwork.yaml.
.4 virtualnetworksubnet.yaml.
.3 gcp/.
.4 computeinstance.yaml.
.4 computenetwork.yaml.
.4 computesubnetwork.yaml.
.3 \_helpers.tpl.
.2 Chart.yaml.
.2 values.schema.json.
.2 values.yaml.
}

\vspace{3.0cm}

\begin{lstlisting}[language=yaml, caption={Helm Template guards example}, label={lst:guards}]
{{ if hasKey .Values "provider" }}
{{ $provider := .Values.provider }}
{{ if eq $provider "azure" }}
...
apiVersion: compute.azure.com/v1api20220301
kind: VirtualMachine
...
\end{lstlisting}


\textbf{VM size selection} is a crucial step in the VM provisioning process.
Helm allows the definition of \textbf{helper functions} which resides in the \textit{\_helpers.tpl} file.
In our case, we defined a helper function called \textbf{\textit{findBestVmSize()}} that takes as input the CPU and RAM requirements of the VM and returns the best provider-specific VM size available.
For instance: a generic requested VM specification could be (4 vCPU, 8 GiB of RAM) and this would be mapped to the ``\textit{Azure Standard\_A4\_v2}'' VM size.
A prerequisite for that is to have a mapping between the tuple (CPU, RAM) and the VM sizes (a string) for each cloud provider.
This mappings are built using the cloud provider documentation and are stored in the Helm chart as a set of utilities files.
Potentially, an automatic way to fetch the available VM sizes for each cloud provider could be implemented in the future.
For our first iteration of the system we used a small subset of all the available instance types (i.e., 5 instance types for each cloud provider).

\subsubsection{Scheduling time waiting logic}
\label{sec:scheduling_time_waiting_logic}

Helm template engine is also leveraged to handle the \textbf{scheduling time waiting logic}.
As previously described in section \ref{sec:limitations_custom_operator}, this logic is crucial for the system due to the fact that cloud provider operators do not support scheduling time metadata in their CRs and therefore as soon as a CR is created the cloud provider operator will provision the resource immediately.
It must be remembered that is not a trivial task to implement this logic in a Kubernetes operator.
In this case, we leverage a set of ``\textbf{guards}'' to effectively block manifest creation until the scheduling time is reached. 
As a result, only when the scheduling time is reached the manifest is created and applied by Helm and finally the resource is provisioned by the cloud provider operator.
As a matter of fact, CDC will periodically perform an \textit{helm upgrade} and if the scheduling time is reached the cloud provider specific resources will be created. 
As a consequence, the specific cloud provider operator will be triggered by the creation of the cloud provider specific resources and will provision the VM.
Listing \ref{lst:sched} shows an example of how guards are used to handle the scheduling time waiting logic and other scheduling constraints.
We also make sure, for instance, that the generic CR contains a ``provider'' field that specifies the cloud provider where the resource should be provisioned.

\begin{lstlisting}[language=yaml, caption={Scheduling time guard}, label={lst:sched}]
{{ if hasKey .Values "provider" }}
{{ $provider := .Values.provider }}
{{ if eq $provider "azure" }}

{{ if hasKey .Values "schedulingTime" }}
{{ $schedulingTime := .Values.schedulingTime | toDate "2006-01-02T15:04:05Z" }}
{{ $now := now }}
@\yellowhl{\{\{{ if \$now.After \$schedulingTime \}\}} }@
...
apiVersion: compute.azure.com/v1api20220301
kind: VirtualMachine
metadata:
  name: {{ .Values.vmName }}
...
\end{lstlisting}

In order to take into account the time needed for the cloud provider operator to provision the resource, additional logic could be added in the template to anticipate the scheduling time (e.g, using a \textit{provisioningTimeOffset}).

\section{Multi-Cloud Integration through Kubernetes Operators}
\label{sec:cloud_providers_operators}

The integration of operators from different cloud providers has enabled the development of an effective \textbf{multi-cloud system}, allowing seamless orchestration and provisioning of cloud resources across various cloud platforms. 
More precisely, the system leverages Kubernetes operators from \textbf{Microsoft Azure}, \textbf{Google Cloud Platform}, and \textbf{Amazon Web Services}.
Each operator, when installed on a Kubernetes cluster, installs a \textbf{set of CRDs} that represent cloud resources specific to the cloud provider.
These CRDs can be used to define cloud resources in a declarative manner, in the form of Kubernetes CRs, allowing users to specify the desired state of the cloud resources they wish to manage.
The other component installed by the operator is the \textbf{controller}, a software module that watches for changes to the CRs in the cluster and takes all the necessary actions to reconcile the actual state with the desired state.
Under the hood the controller interacts with the cloud provider's API to provision, update, and delete cloud resources.
Kubernetes operators work on the principle of \textbf{continuous reconciliation}, ensuring, in this case, that the desired state of the system, as defined by users, aligns with the actual state of provisioned cloud resources. 
In particular, operators act as controllers that monitor (\textit{watch}), adjust, and manage external cloud resources within a Kubernetes environment. 
Inside the Kubernetes cluster lie the \textbf{real-time representations of the provisioned cloud resources}, which are managed by the operators.
It must be noted that different cloud providers adopt \textbf{different design choices} for their Kubernetes operators and more in general for their overall cloud infrastructure management. 
Therefore, for the creation of logically similar resources, like a virtual machine, the structure and the field of the resources can be different. 
These resources typically include:
\begin{itemize}[itemsep=0.2pt, topsep=1pt]
  \item[$\bullet$] Compute resources (e.g., VM instances, VM templates)
  \item[$\bullet$] Networking components (e.g., virtual networks, subnets, security groups)
  \item[$\bullet$] Storage allocations (e.g., persistent volumes, cloud disks)
  \item[$\bullet$] Access management (e.g., resource groups, roles, authentication credentials) \\
\end{itemize}

For the purpose of this work we defined a \textbf{baseline infrastructure} for each cloud provider taken into account in order to have a common ground for the system to work. 
This baseline infrastructure is composed by the minimum set of resources needed for VM provisioning.
Each public cloud provider has its complexities and nuances when it comes to managing cloud resources.
In the following sections, we provide an overview of the minimum set of resources needed for VM provisioning on each cloud provider, as well as some of the specific configurations required for each resource.
We deem useful to provide a table summarizing some of the key fields that are needed for VM provisioning.
These fields are labeled with an ID which will be attached to the resources' listings in the following sections.

\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|}
  \hline
  \textbf{ID} & \textbf{VM field} \\ \hline
  1           & Scheduling region \\ \hline
  2           & VM size           \\ \hline
  3           & Operating System           \\ \hline
  \end{tabular}
  \caption{Key fields for VM provisioning}
  \label{tab:vm_fields}
\end{table}

\subsection{Azure Kubernetes Operator}

Microsoft Azure provides a Kubernetes operator called \textbf{Azure Service Operator v2} (ASO).
Currently, ASO supports more than 150 different Azure resources one of which is the \textbf{Azure VM}.
An example of a Azure VM CR is the one illustrated in listing \ref{lst:azure}.
The minimum set of resources needed for VM provisioning on Azure through Azure Service Operator is:

\begin{itemize}[itemsep=0.2pt, topsep=1pt] 
\item[$\bullet$] Virtual Network 
\item[$\bullet$] Virtual Network Subnet
\item[$\bullet$] Network Interface
\item[$\bullet$] Virtual Machine
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\linewidth]{images/azure.png}
\caption{Minimum set of Azure resources for VM provisioning}
\label{fig:azure}
\end{figure}

\begin{lstlisting}[language=yaml, caption={Azure VM Custom Resource}, label={lst:azure}]
apiVersion: compute.azure.com/v1api20220301
kind: VirtualMachine
metadata:
  name: {{ .Values.vmName }}
  namespace: {{ .Values.namespace | default "greenops" }}
spec:
  hardwareProfile:
   @\yellowhl{    vmSize: \{\{{ \$vmSize \}\}} \#[2] }@
   @\yellowhl{    location: \{\{{ .Values.schedulingLocation \}\}} \#[1] }@
  networkProfile:
    networkInterfaces:
    - reference:
        group: network.azure.com
        kind: NetworkInterface
        name: {{ .Values.vmName }}-{{ $av.networkInterface.suffix }}
  osProfile: {{ $av.virtualMachine.osProfile | toYaml | nindent 4 }}
  owner:
    armId: {{ $av.resourceGroup.armId }}
  storageProfile:
   @\yellowhl{    imageReference: \#[3] }@
      publisher: Canonical
      offer: 0001-com-ubuntu-server-jammy
      sku: 22_04-lts
      version: latest
\end{lstlisting}

\subsection{GCP Operator}

Google Cloud Platform provides a Kubernetes operator called \textbf{GCP Config Connector}.
The name of the virtual machine resource in GCP is \textbf{ComputeInstance}.
An example of a GCP ComputeInstance CR is the one illustrated in listing \ref{lst:gcp}.
For what concerns the GCP Operator, the minimum set of resources needed for VM provisioning is:

\begin{itemize}[itemsep=0.2pt, topsep=1pt] 
  \item[$\bullet$] ComputeNetwork
  \item[$\bullet$] ComputeSubnetwork
  \item[$\bullet$] ComputeInstance
  \end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\linewidth]{images/gcp.png}
\caption{Minimum set of GCP resources for VM provisioning}
\label{fig:gcp}
\end{figure}

We can mention the fact that some fields on GCP resource manifests are based on regions and some fields are based on avavilability zones.
In order to select a zone, a helper function is used to select a zone based on the region.

\begin{lstlisting}[language=yaml, caption={GCP ComputeInstance Custom Resource}, label={lst:gcp}]
  apiVersion: compute.cnrm.cloud.google.com/v1beta1
  kind: ComputeInstance
  metadata:
    name: {{ .Values.vmName }}
    namespace: {{ .Values.namespace | default "greenops" }}
  spec:
   @\yellowhl{    machineType: \{\{{ \$vmSize \}\}} \#[2] }@
   @\yellowhl{    zone: \{\{{ \$zone \}\}} \#[1] }@
    bootDisk:
      initializeParams:
        size: 24
        type: pd-ssd
        sourceImageRef:
         @\yellowhl{    external: debian-cloud/debian-11 \#[3] }@
    networkInterface:
      - subnetworkRef:
          name: {{ .Values.vmName }}-subnetwork
        aliasIpRange:
          - ipCidrRange: /24
            subnetworkRangeName: cloudrange
\end{lstlisting}

\subsection{AWS Operator}

AWS provides an entire collection of operators that are part of the AWS controllers for Kubernetes (ACK) project.
Each controller is responsible for managing a specific AWS service, such as EC2, EBS, RDS, S3, and more. In the context of this research, the EC2 controller is used to manage virtual machines (called EC2 Instances) provisioning on AWS.
An example of a AWS EC2 Instance Custom Resource is the one illustrated in listing \ref{lst:aws}.

\vspace{2cm}
The minimum set of resources needed for VM provisioning is:
\begin{itemize}[itemsep=0.2pt, topsep=1pt] 
\item[$\bullet$] VPC
\item[$\bullet$] Subnet
\item[$\bullet$] EC2 Instance
\end{itemize}

\begin{figure}[t]
\centering
\includegraphics[width=0.75\linewidth]{images/aws.png}
\caption{Minimum set of AWS resources for VM provisioning}
\label{fig:aws}
\end{figure}

\subsubsection{Provider specific configurations}

As described at the beginning of this section, the implementation approach adopted in our system ensures compatibility with diverse cloud provider design choices. Cloud providers may impose different constraints and best practices when managing Kubernetes-native resources, and the system is designed to adapt to these variations seamlessly.
One notable design choice observed with the AWS operator is the restriction on referencing some Kubernetes objects inside a CR manifest. 
This limitation means that developers cannot directly link a resource (e.g., a VM) to another Kubernetes object (e.g., a Subnet) using built-in object references.
To overcome this limitation, our system leverages \textbf{Helm’s \textit{lookup function}}, which dynamically retrieves Kubernetes object details at runtime. This method allows us to fetch required parameters without directly referencing Kubernetes objects in the CR, ensuring compatibility with the AWS operator’s design constraints. The following example demonstrates how the lookup function can be used to resolve subnet IDs dynamically and inject them into the CR manifest. \newline

\begin{lstlisting}[language=yaml, caption={Helm Lookup example: dynamically resolving SubnetIDs}, label={lst:helm_lookup}, float=htpb]
...
apiVersion: ec2.services.k8s.aws/v1alpha1
kind: Instance
metadata:
  name: {{ .Values.vmName }}
  namespace: {{ .Values.namespace | default "greenops" }}
...
spec:
  ...
  subnetID: {{ (lookup "ec2.services.k8s.aws/v1alpha1" "Subnet" (.Values.namespace | default "greenops") (printf "%s-subnet" .Values.vmName)).status.subnetID }}
...
\end{lstlisting}

The Helm lookup function can be used to look up resources in a running cluster and its synopsis is: ``lookup apiVersion, kind, namespace, name'' \cite{helm_lookup}.
In listing \ref{lst:helm_lookup}, the Helm lookup function retrieves the subnetID from a Subnet Custom Resource dynamically, based on the VM name and namespace. Then, the subnetID is injected into the Instance Custom Resource manifest, ensuring that the VM is provisioned in the correct subnet.
An example by the same AWS Operator where instead a direct reference to a resource is allowed is the one illustrated in listing \ref{lst:aws_direct_reference}. \\

\begin{lstlisting}[language=yaml, caption={AWS Operator direct reference example}, label={lst:aws_direct_reference}, float=htpb]
...
apiVersion: ec2.services.k8s.aws/v1alpha1
kind: Subnet
metadata:
  name: {{ .Values.vmName }}-subnet
...
spec:
  vpcRef: 
    from: 
      name: {{ .Values.vmName }}-vpc
      namespace: {{ .Values.namespace | default "greenops" }}
...
\end{lstlisting}

In the case of Listing \ref{lst:aws_direct_reference}, the Subnet CR manifest directly references in a convenient way the VPC CR using its name and namespace since the operator is designed to support this type of relationship. 
As explained before, this is determined by operator design choices but our system is able to handle both scenarios.
\newline
Another important aspect to consider is that, when launching an Amazon EC2 instance, specifying an AMI is \textbf{mandatory}. 
An Amazon Machine Image (AMI) is a pre-configured image that provides the necessary software environment to set up and boot an Amazon EC2 instance \cite{aws_AMIs}. 
In other words, AMIs serve as a blueprint for launching VMs in AWS.
The AMI must be compatible with the chosen EC2 instance type, ensuring that the selected image supports the required hardware and software configurations.
The following attributes define an AMI:
\begin{itemize}[itemsep=0.2pt, topsep=1pt] 
  \item[$\bullet$] Region: AMIs are region-specific
  \item[$\bullet$] Operating System: Determines the base OS installed on the AMI (e.g., Ubuntu, RHEL).
  \item[$\bullet$] Processor Architecture: e.g., x86, ARM
  \item[$\bullet$] Root Device Type: Specifies whether the AMI uses an EBS-backed volume (Elastic Block Store) or Instance Store for storage.
  \item[$\bullet$] Virtualization Type: Defines whether the AMI supports paravirtual (PV) or hardware virtual machine (HVM) instances.
\end{itemize}

The most important attribute for our system is the \textbf{Operating System} since it determines the software environment for the VM.
For the purpose of this research, only \textbf{Ubuntu-based AMIs} have been considered for provisioning virtual machines. 
Official Ubuntu AMIs were collected from a dedicated Ubuntu repository.
Nonetheless, this does not limit the applicability of this architecture since VMs with other operating systems could be also used.
In order to select the most suitable AMI for a given VM, the system leverages a custom helper function to dynamically select the appropriate AMI ID based on the region and other parameters specified in the VmTemplate Kubernetes Custom Resource.

\vspace{5.0cm}

\begin{lstlisting}[language=yaml, caption={AWS EC2 Instance Custom Resource}, label={lst:aws}]
apiVersion: ec2.services.k8s.aws/v1alpha1
kind: Instance
metadata:
  name: {{ .Values.vmName }}
  namespace: {{ .Values.namespace | default "greenops" }}
  annotations:
   @\yellowhl{    services.k8s.aws/region: \{\{{ .Values.schedulingLocation \}\}} \#[1] }@
spec:
 @\yellowhl{    imageID: \{\{{ \$imageID\}\}} \#[3] }@
 @\yellowhl{    instanceType: \{\{{ \$vmSize \}\}} \#[2] }@
  subnetID: {{ (lookup "ec2.services.k8s.aws/v1alpha1" "Subnet" (.Values.namespace | default "greenops") (printf "%s-subnet" .Values.vmName)).status.subnetID }}
\end{lstlisting}

\section{Kubernetes Mutating Webhook Configuration}

Kubernetes admission control is a set of mechanisms that control how Kubernetes API requests are processed by the API server before they are persisted in the cluster \cite{kubernetes_dynamic_admission_control}.
In this context, in addition to standard, compiled-in admission plugins, Kubernetes supports the use of additional admission plugins that are effectively extensions of the system and run as \textbf{webhooks} configured at runtime \cite{kubernetes_dynamic_admission_control}. 
This means that the admission control logic can be extended dynamically without the need to recompile the Kubernetes API server or other Kubernetes components. 
Changes are applied at runtime to the running Kubernetes cluster, making the system more flexible and adaptable.
Said plugins can be used to enforce custom policies and perform additional validation and mutation of Kubernetes objects before they are persisted in the cluster.
We can classify webhooks in two categories: validating and mutating webhooks.
Validating webhooks are used to validate the object and reject it if it does not meet the validation criteria.
Mutating webhooks are used to modify the object before it is persisted in the cluster.
We must highlight the difference of two entities: the webhook configuration and the actual webhook server which performs mutation or validation.
The latter could be a custom server to apply custom mutation or validation logic or it could be a service ready to use out of the box like Open Policy Agent \cite{opa_docs}.

Figure \ref{fig:k8s_admission_control} shows the Kubernetes admission control flow with the addition of mutating and validating webhooks.
The flow can be summarized as follows:
\begin{enumerate}[itemsep=0.2pt, topsep=1pt]
  \item The Kubernetes API server receives an API request.
  \item Authentication and authorization phase take place.
  \item Mutating webhooks are called first. If any of them returns an error, the request is rejected.
  \item The Object Schema validation phase is executed.
  \item Validating webhooks are called. If any of them returns an error, the request is rejected.
  \item The object is persisted in the cluster.
\end{enumerate}

\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{images/k8s_admission.png}
  \caption{Kubernetes Admission Control}
  \label{fig:k8s_admission_control}
\end{figure}

A webhook service can be either a Kubernetes deployment with the related service inside the cluster like seen in the image \ref{fig:k8s_webhook_example} or can be a service deployed outside the cluster as long as it is reachable by the Kubernetes API server.

Figure \ref{fig:k8s_webhook_example} shows an example of a Kubernetes mutating webhook. 
The webhook server is responsible for receiving the AdmissionReview request, applying custom logic, and returning an AdmissionReview response to the Kubernetes API server describing the mutation to be applied to the resource.
The custom logic in this simple example is to add a label ``mutated: true" to the resource.

\begin{figure}[t]
  \centering
  \includegraphics[width=1\linewidth]{images/k8s_webhook_example.png}
  \caption{Kubernetes Mutating Webhook example \cite{kubernetes_mutating_webhook_example}}
  \label{fig:k8s_webhook_example}
\end{figure}

In our system, we configured a \textbf{\textit{MutatingWebhookConfiguration}} to intercept CREATE and UPDATE Kubernetes API requests for VmTemplate CRs.
In this case, the Kubernetes Mutating Webhook server that is the target of the MutatingWebhookConfiguration is OPA.
OPA is used to assign scheduling decisions to VmTemplates based on policies. 
OPA then sends back to the API server the mutation (patches) to be applied to the VmTemplate CR.

\section{Open Policy Agent integration}

In the context of our system, OPA and the Policy as Code paradigm are mainly leveraged to define \textbf{policies for workload scheduling}: encoding the output of a \textbf{scheduling decision} coming from an external GreenOps Scheduler and ensuring compliance with \textbf{additional policies} related to latency requirements and legal constraints (QoS, data residency, etc.).
In particular, OPA assumes the role of a \textbf{Kubernetes Mutating Webhook server} to apply scheduling decisions to VmTemplate Custom Resources based on policies.

\subsection{OPA architecture overview}

As mentioned in the background chapter, one common approach to integrating OPA into a software system is by deploying it as a host-level daemon. 
The latter is essentially a lightweight server that processes policy queries via HTTP requests. 
This setup, represented in figure \ref{fig:opa_architecture} allows services to offload policy decision-making to OPA in a scalable and efficient manner since the two entities are not tightly coupled.

A standard OPA deployment consists of three main components:

\begin{enumerate}[itemsep=0.2pt, topsep=1pt]
  \item \textbf{OPA Server}: The core service that evaluates policy queries and returns decisions based on defined rules, contextual data and input data.
  \item \textbf{OPA Policies}: Rules written in Rego language that define the logic to be enforced.
  \item \textbf{Data}: Optional contextual information, typically structured in JSON format, that policies use to make informed decisions along with input data.
\end{enumerate}

To facilitate deployment and management, Rego policies and associated contextual data are packaged into \textbf{policy bundles}, as described in section \ref{sec:opa_policies}. These bundles enable version-controlled, centralized policy distribution, ensuring consistency and maintainability across distributed environments. \newline

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{images/OPA.png}
\caption{OPA architecture}
\label{fig:opa_architecture}
\end{figure}

%OPA accepts arbitrary structured data as input and can generate arbitrary structured data as output. 
%This flexibility allows OPA to be used in a wide range of use cases, as described above.

\subsection{OPA and external data sources}
\label{sec:opa_external_data}

In order to get data from external sources, OPA provides several options that can be chosen based on size and frequency of update \cite{opa_external_data}.
For the use case of this work, external data to be pulled is the \textbf{scheduling decision} from the GreenOps Scheduler in a synchronous way.
The most suitable option is to \textbf{pull data during policy evaluation} since scheduling decisions must be made in real-time.
The data is pulled from the GreenOps Scheduler through OPA built-in functions and libraries \cite{opa_external_data}.
Listing \ref{lst:opa_external_data} shows an example of how the external data pull is implemented in Rego.

%\begin{lstlisting}[language=sh, caption={GreenOps scheduler HTTP request example}, label={lst:scheduler_req}]
%#!/bin/bash
%curl -X POST -H "Content-Type: application/json" \
%%  -d '{
%      "number_of_jobs" : 1,
%      "eligible_regions": ["FR", "IT", "FI"],
%      "deadline": "2025-06-09T10:00:00",
%      "duration": 6,
%      "cpu": 4,
%      "memory": 8,
%      "req_timeout": 180
% }' http://greenops-scheduler.greenops-scheduler-system.svc.cluster.local/scheduling
%
%# Response: 
%# {"scheduling_location":"FR","scheduling_time":"2025-02-06T17:42:11Z"}
%\end{lstlisting}

\vspace{3.0cm}
\begin{lstlisting}[language=rego, caption={OPA external data pull}, label={lst:opa_external_data}]
scheduler_url := opa.runtime().env.SCHEDULER_URL # GreenOps Scheduler URL

scheduling_details := http.send({
	"method": "POST",
	"url": scheduler_url,
	"body": {
		"number_of_jobs": 1, # currently only one job (workload (VM)) can be scheduled at a time
		"eligible_regions": eligible_electricity_maps_regions,
		"deadline": deadline,
		"duration": duration, 
		"cpu": cpu,
		"memory": memory,
		"req_timeout": 10 # seconds, scheduler wants to know the timeout to tune the execution time of the optimization
	},
	"timeout": "10s",
	"headers": {
		"Content-Type": "application/json"
	},
	"max_retry_attempts": 3, # retry 3 times in case of failure
})
\end{lstlisting}

\subsection{OPA integration with Kubernetes}

In the context of the Kubernetes admission control mechanism, policy enforcement is handled by the \textbf{Kubernetes API server} itself. 
OPA makes the policy decisions when queried by the admission controller, but the actual enforcement (namely allowing, denying, modifying requests) is executed by Kubernetes' built-in admission control mechanisms. 
This is effectively one of the core design principles of OPA: to provide policy decisions to external systems, which then enforce the decisions based on their own logic \cite{opa_kubernetes_primer}.
This workflow is represented in figure \ref{fig:webhook_opa} where the \textbf{AdmissionReview request} and \textbf{AdmissionReview response} are respectively input and output of the whole OPA section. 
The API Server sends the entire Kubernetes object in the webhook request to OPA \cite{opa_kubernetes_primer}.
The Kubernetes API server will then use the received AdmissionReview response for its decision. 
%\cite{opa_philosophy}
% https://www.openpolicyagent.org/docs/latest/kubernetes-tutorial/

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{images/webhook.png}
\caption{Kubernetes mutating webhook and OPA integration}
\label{fig:webhook_opa}
\end{figure}

In a Kubernetes deployment, an \textbf{OPA server pod} typically consists of the following containers:
\begin{itemize}[itemsep=0.2pt, topsep=1pt]
  \item[$\bullet$] OPA server container
  \item[$\bullet$] \textbf{kube-mgmt} container \\
\end{itemize} 

The kube-mgmt container functions as a \textbf{sidecar container} within a Kubernetes pod. The sidecar container pattern is a common Kubernetes design paradigm in which auxiliary containers run alongside the main application container within the same pod. These additional containers serve to enhance, extend, or support the primary application's functionality without modifying its core logic \cite{sidecar_containers}. 
The primary responsibility of kube-mgmt is to replicate Kubernetes resources into the OPA instance (OPA container). This operation is essential for OPA to access and evaluate policies based on real-time cluster state, enabling dynamic policy enforcement. By synchronizing these resources, kube-mgmt ensures that OPA has an up-to-date view of relevant Kubernetes objects.
This is especially useful to enforce policies that deals with naming conflicts, where OPA needs to check existing names in the cluster for the decision \cite{kube-mgmt}.
Additionally, it allows for loading policies directly from the Kubernetes cluster by retrieving them in the form of ConfigMaps. This feature is particularly useful when policies need to be dynamically updated based on the current state of the cluster \cite{kube-mgmt}. However, in the system described in this thesis, this latter feature is not employed in the current implementation as we are using policy bundles for policy distribution.

In the current system configuration, the kube-mgmt container is deployed to facilitate resource replication, ensuring that Kubernetes resources, namely VmTemplate resources, are synchronized with the OPA instance. 
However, at present, no policy requires interrogation of VmTemplate resources that are already present in the system.
Looking ahead, future policies could leverage VmTemplate resource information to enforce naming conflict resolution, quota management, or additional constraints.

\subsection{OPA policies}
\label{sec:opa_policies}

%example of a policy:
%(https://www.openpolicyagent.org/docs/latest/kubernetes-primer/)

As OPA official documentation describes, when the Kubernetes AdmissionReview request from the webhook arrives, it is bound to the \textbf{OPA input document} and generates the default, ``root", decision: \textit{system.main} \cite{opa_docs}.
The root policy, in the case of Kubernetes admission control, is responsible for generating the AdmissionReview response in accordance with the Kubernetes API specifications. 
Indeed, it is the duty of the policy developer to write Rego code that produces a \textbf{well-formed AdmissionReview response}, ensuring that the OPA server can then correctly communicate its decision to the Kubernetes admission controller \cite{opa_docs}.
OPA policies are compiled before being evaluated, and any errors that occur during compilation are reported back to the caller.
An example of these errors is when there are merge errors on contextual data \cite{opa_docs}.
For the purpose of this thesis, it is deemed useful to show one of the simplest and common example of a OPA policy in the \textbf{Kubernetes admission control context}. 
That is: to ensure all images for Kubernetes pods come from a trusted registry, in this example \textit{unitn.it}.
It is important to note that, in this case, due to the simplicity of the policy, no additional contextual data in JSON format is required while in a standard scenario, data is used to provide additional context to the policy evaluation process. \newline

Listing \ref{lst:rego_policy_example} shows the Rego policy that enforces the rule described above while listing \ref{lst:rego_root_policy} shows the root policy that generates the AdmissionReview response.
Listings \ref{lst:admission_review_request} and \ref{lst:admission_review_response} show respectively an example of an AdmissionReview request and response. \newline

\begin{lstlisting}[language=Rego, caption={Rego policy for Pods registry}, label=lst:rego_policy_example]                              
deny contains msg if {                                                      
    input.request.kind.kind == "Pod"                                        
    image := input.request.object.spec.containers[_].image                  
    not startswith(image, "unitn.it/")                                     
    msg := sprintf("image '%v' comes from untrusted registry", [image])     
}
\end{lstlisting}

\vspace{5cm}

\begin{lstlisting}[language=Rego, caption={Rego ``root" policy (system.main)}, label=lst:rego_root_policy]
main := {
	"apiVersion": "admission.k8s.io/v1",
	"kind": "AdmissionReview",
	"response": response,
}
default uid := ""
uid := input.request.uid
response := {
	"allowed": false,
	"uid": uid,
	"status": {"message": reason},
} if {
	reason := concat(", ", admission.deny)
	reason != ""
}
else := {"allowed": true, "uid": uid}
\end{lstlisting}

\lstset{style=jsonstyle}
\begin{lstlisting}[caption={AdmissionReview request}, label={lst:admission_review_request}]
{
    "apiVersion": "admission.k8s.io/v1",
    "kind": "AdmissionReview",
    "request": {
        "kind": {
            "group": "",
            "kind": "Pod",
            "version": "v1"
        },
        "object": {
            "metadata": {
                "name": "myapp"
            },
            "spec": {
                "containers": [
                    {
                        "image": "bitnami/node:22",
                        "name": "nodejs"
                    }
                ]
            }
        }
    }
}
\end{lstlisting}

\lstset{style=jsonstyle}
\begin{lstlisting}[caption={AdmissionReview response}, label={lst:admission_review_response}]
{
    "apiVersion": "admission.k8s.io/v1",
    "kind": "AdmissionReview",
    "response": {
        "allowed": false
        "status": {
            "message": "image 'bitnami/node:22' comes from untrusted registry"
        }
    }
}
\end{lstlisting}

Therefore, in this specific case, the creation of the Kubernetes pod will be \textbf{denied}. 
OPA is responsible for \textbf{decision-making}, determining that the request do not comply with the defined policies, while the Kubernetes API server, using the AdmissionReview response generated by OPA, handles \textbf{policy enforcement}, effectively rejecting the CREATE request since it violates the specified rules.

\subsection{OPA policy bundles}
\label{sec:opa_bundles}

An OPA policy bundle is a collection of policies and optional associated contextual data. More precisely, a bundle is a standardized way to package policies, facilitating version control and distribution \cite{opa_bundles}. As a matter of fact, a single policy bundle can be potentially used by multiple OPA instances.
A policy bundle mainly consists of:
\begin{itemize}[itemsep=0.2pt, topsep=1pt]
  \item[$\bullet$] \textbf{Rego policy files} defining the logic.
  \item[$\bullet$] \textbf{Data files} (in JSON or YAML format) containing contextual information required for policy evaluation (e.g., cloud region mappings).
\end{itemize} 

Policy bundles can be distributed through a variety of mechanisms such as remote HTTP servers (e.g., NGINX) and object storage services (e.g., Amazon S3, Google Cloud Storage, Azure Blob Storage) \cite{opa_bundles}.
One of the most convenient approaches is packaging policy bundles as \textbf{OCI (Open Container Initiative) images} \cite{oci} and this is the approach adopted in the system described in this thesis.
Once packaged as OCI images, policy bundles can be pulled by OPA servers from a container registry at predefined and configurable time intervals. 
This allows policy updates to be deployed in OPA \textbf{without requiring manual intervention} or \textbf{service restarts}, ensuring that enforcement mechanisms remain up to date with the latest compliance requirements identified and implemented by the organization. 
This is crucial for instance when dealing with \textbf{critical security policies} that need to be updated frequently, maybe in response to the discoveries of new Common Vulnerabilities and Exposures (CVEs). 
To ensure continuous policy enforcement while maintaining high operational efficiency, a CI/CD approach is adopted for policy management in the context of our system.
As a matter of fact, policies are maintained in a \textbf{version-controlled hosted repository} (i.e., on GitHub), where updates like tagging (``git tag'') trigger an automated pipeline (e.g., using GitHub Actions) responsible for building, packaging into a OCI image, and publishing the policy bundle to a container registry (e.g., Docker Hub).
One of the major advantages of this approach is the ability to dynamically update policies without requiring OPA pods to restart as there is an \textbf{hot-reload} of policies done at application level by OPA (``loaded on the fly'') \cite{opa_bundles}. 
This is particularly useful in production environments where service availability is critical and downtime must be minimized. 
The overall process of policy distribution adopted and implemented in our system is illustrated in figure \ref{fig:opa_bundles}.

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{images/opa_bundles.png}
\caption{OPA policy bundles}
\label{fig:opa_bundles}
\end{figure}

By leveraging OCI images for policy distribution and implementing fully automated CI/CD pipelines, this methodology ensures that policy enforcement remains consistent, up to date, and highly available across all OPA instances. 
This approach aligns with modern and standard DevOps practices, enabling organizations to maintain a high level of security and compliance without compromising operational efficiency.

\subsection{Latency policy}

A representative example of a policy aligned with Service Level Objectives (SLOs) or Service Level Agreements (SLAs) is the latency policy described in this section.
Given an \textbf{origin region} and a \textbf{maximum latency threshold} (expressed in milliseconds), the objective is to determine a \textbf{set of eligible regions} where the inter-regional latency between the origin and each region in the set is equal to or below the specified threshold.

Enforcing such constraints helps mitigate the so-called ``\textbf{black hole phenomenon}" in the GreenOps use case, where all virtual machines (VMs) would otherwise be scheduled in a region with generally low carbon intensity, without considering additional constraints or performance requirements.
As a matter of fact it would be possible to simply schedule all VMs in the region with the lowest carbon intensity, but this would not be a viable solution in a real-world scenario where performance is a critical factor.
Therefore, by adding a latency constraint, the greenest region could not be selected if it does not meet the performance requirements.

By incorporating similar performance-aware policies, organizations can achieve a balance between environmental impact, performance, and service reliability.
The proposed flexible system enables organizations to fine-tune these factors according to their specific requirements or those of their users. 
This policy demonstrates the flexibility of OPA in handling diverse compliance scenarios. It is the responsibility of the policy developer to design an appropriate strategy for encoding relevant information into \textbf{well-structured JSON data models}, e.g., a latency matrix. Proper structuring ensures efficient policy evaluation, maintainability and extendability.

Figure \ref{fig:latency_matrix} illustrates a small example (4 regions subset) of a latency matrix, where each cell represents the latency between two regions. The matrix can be encoded in JSON format as illustrated in listing \ref{lst:latency_matrix_example}, allowing for easy integration with OPA policies. The ``Latency policy'' then uses this matrix to determine eligible regions based on the origin region and maximum latency threshold.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.575\linewidth]{images/latency_heatmap.png}
  \caption{Latency matrix example (Azure regions subset)}
  \label{fig:latency_matrix}
\end{figure}

\lstset{style=jsonstyle}
\begin{lstlisting}[caption={Latancy matrix example encoded in JSON format}, label={lst:latency_matrix_example}, float=t]
{
  "italynorth": {
      "italynorth": 0,
      "japaneast": 249,
      "francecentral": 24,
      "westus": 165
  },
  "japaneast": {
      "italynorth": 252,
      "japaneast": 0,
      "francecentral": 252,
      "westus": 108
  },
  "francecentral": {
      "italynorth": 25,
      "japaneast": 251,
      "francecentral": 0,
      "westus": 150
  },
  "westus": {
      "italynorth": 165,
      "japaneast": 108,
      "francecentral": 150,
      "westus": 0
  }
}

\end{lstlisting}

For what concern data sources, cloud-specific inter-regional latency data were obtained in different ways for each cloud provider.
Microsoft Azure provides monthly ``Percentile P50 round trip times'' between Azure regions in its documentation \cite{azure_network_latency}. 
This data was automatically scraped, merged and used to build the latency matrix for Azure regions.
For AWS a similar approach was used, but the data was obtained from a third-party website that provides latency data between AWS regions calculated using AWS Lambda functions \cite{cloudping}.
For Google Cloud Platform, no official data was found, so the latency matrix was built using synthetic data that simulates realistic latencies between regions, similar to other cloud providers.

\subsection{GDPR policy}

Another policy configured in the system is the ``GDPR Policy'', which ensures that virtual machines (VMs) are deployed in cloud regions that reside in countries of the European Union (EU). 
The policy is based on the principle of \textbf{set intersection}. One set consists of the eligible regions determined by other constraints, such as latency requirements. The other set includes cloud provider regions that are physically located within European Union countries.
The intersection of these two sets defines the final list of allowed deployment regions, restricting workloads to EU-based data centers.
Since each cloud provider has its own regional distribution, the list of EU-compliant regions is provider-specific and is encoded as contextual data in JSON format. This allows for flexibility and easy updates when cloud providers introduce new regions.

It must be noted that this policy is \textbf{not intended to be a comprehensive GDPR compliance solution}, but rather a basic example of how OPA can enforce \textbf{data residency requirements in a multi-cloud environment}. Organizations with more stringent GDPR compliance needs should consider additional measures.

\subsection{Scheduling outcome policy}
\label{sec:scheduling_outcome_policy}

In the context of this work, the ``Scheduling outcome policy'' is a policy that determines the scheduling decision for a given workload based on the output of the GreenOps Scheduler which is queried in real-time, as described in section \ref{sec:opa_external_data}.
The inputs are defined as follows:
\begin{itemize}[itemsep=0.2pt, topsep=0.2pt]
  \item \{CPU, RAM, duration, deadline, max latency\}: set at request time
  \item eligible cloud providers: set at request time or in policies (configurable)
  \item origin region: set in policies
  \item GDPR compliancy: set in policies
  \item inter-region latency matrix: stored in policy data
\end{itemize}
%\vspace{2cm}
While the outputs are:
\begin{itemize}[itemsep=0.2pt, topsep=0.2pt]
  \item provider
  \item schedulingLocation
  \item schedulingTime
\end{itemize}

It is therefore a policy that has the duty to mutate (patch) the VmTemplate Kubernetes custom resource (Generic VM), adding the scheduling information (provider, schedulingLocation, schedulingTime) to the resource.
According to Kubernetes documentation, this can be done using ``\textit{patch}'' and ``\textit{patchType}" fields in the AdmissionReview response \cite{kubernetes_dynamic_admission_control}.
The ``\textit{patchType}'' field must be ``\textit{JSONPatch}'' and the ``\textit{patch}'' field must contain a base64-encoded array of JSON patch operations to be applied to the resource.
JSON Patch is a format for describing changes to a JSON document which avoids the need to send the entire document when only a part of it has changed. 
Effectively, only deltas are sent back to the requester which are themselves JSON documents.
The format is defined in RFC 6902 from the IETF \cite{json_patch}.
As an example, a single patch operation is the one shown in listing \ref{lst:json_patch}, where a new field ``schedulingTime'' is added to the resource.
It must be said that whether the provider is randomly chosen from a subset, chosen due to additional logic or chosen by the user at request time is not a concern of the policy itself, but this choice is made by the system designer and implemented in the policy.

\begin{lstlisting}[language=rego, caption={JSON Patch example}, label={lst:json_patch}]
# schedulingTime is data coming from the GreenOps Scheduler
patchCode = {
	"op": "add",
	"path": "/spec/schedulingTime",
	"value": schedulingTime,
}
\end{lstlisting}

\subsection{OPA Data mapping}
\label{sec:opa_data_mapping}

OPA is flexible enough to handle \textbf{data mapping operations} between different data models, enabling seamless integration with external systems. 
In our GreenOps system, data mapping is essential for translating between ElectricityMaps regions and cloud provider regions. 
At some point in the system this mapping needed to be done and we deemed that inside the OPA policies was the best place to
do it.
In particular, these mappings are needed since the GreenOps scheduler knows only the notion of ElectricityMaps regions, and does not possess the knowledge of cloud provider regions. 
Therefore, a mapping is needed to translate the ElectricityMaps regions to cloud provider regions and vice versa.

The entire data mapping process can be broken down into the following steps:
\begin{enumerate}[itemsep=0.2pt, topsep=1pt]
  \item Cloud provider selection (e.g., Azure, AWS, GCP): this determines the set of cloud provider regions.
  \item Latency filtering: this step can only be done with cloud provider-specific latencies and determines the eligible regions.
  \item GDPR compliance filtering (if enabled): this step ensures that only regions in the EU are selected.
  \item ElectricityMaps regions mapping: this step maps the eligible cloud provider regions to ElectricityMaps regions.
  \item Scheduling outcome: this step determines the scheduling region (ElectricityMaps region) based on the output of the GreenOps Scheduler.
  \item The ElectricityMaps region is then mapped back to the cloud provider region as final step. \newline
\end{enumerate}

This process is illustrated in figure \ref{fig:data_mapping} and the Rego code in listing \ref{lst:rego_data_mapping} illustrates the Rego functions used for data mapping between cloud provider regions and ElectricityMaps regions. \newline

\begin{figure}[t]
  \centering
  \includegraphics[width=0.975\linewidth]{images/data_mapping.png}
  \caption{OPA Data mapping}
  \label{fig:data_mapping}
\end{figure}

\vspace{5cm}

\begin{lstlisting}[language=Rego, caption=Rego data mapping, label=lst:rego_data_mapping]
# Utility functions to map between cloud provider regions 
# and ElectricityMaps regions

map_to_electricitymaps(eligible_regions, provider) = em_regions if {
    em_regions := {
        region.ElectricityMapsName |                             
        some eligible_region;                       
        some region;                                
        eligible_region = eligible_regions[_];      
        region = data[provider].cloud_regions[_];   
        region.Name == eligible_region             
        region.ElectricityMapsName != ""            
        region.ElectricityMapsName != "Unknown"
    }
}

map_from_electricitymaps(em_region, provider) = cloud_region if {
    some region;                              
    region = data[provider].cloud_regions[_];   
    region.ElectricityMapsName == em_region;    
    cloud_region := region.Name 
}

\end{lstlisting}

\subsection{OPA role within the system}

In this section we describe the integration of OPA in the system.
The entire architecture of the system and the integration of OPA is illustrated in figure \ref{fig:architecture}.
OPA has the role of \textbf{mutating webhook server} which is consulted by the Kubernetes API server when a CREATE or UPDATE operation is performed on a VmTemplate Custom Resource.

The OPA server is responsible for evaluating the policies and returning the AdmissionReview response to the API server.
The AdmissionReview response contains the decision of the policy evaluation (i.e. scheduling outcome) and the JSON patch operations to be applied to the VmTemplate resource by the Kubernetes API server.
OPA is periodically polling the policy bundles from an external container registry (e.g., DockerHub) to ensure that the policies are up to date.
The main policy, namely the ``scheduling outcome policy'', is responsible for determining the scheduling decision based on the output of the GreenOps Scheduler called in real-time at request time, as described in section \ref{sec:scheduling_outcome_policy}.
OPA is also responsible for data mapping operations between ElectricityMaps regions and cloud provider regions, as described in section \ref{sec:opa_data_mapping}.
The system is designed to be highly flexible and extensible, allowing for the addition of new policies and data mappings as needed.

\subsubsection{Day 2 operations}
\label{sec:day2_operations}

What was analyzed so far can be defined as ``Day 1'' optimization, i.e., the optimization of resources at deployment time.
We can define as \textbf{\textit{``Day 2 operations''}} all the management operations that are performed after the deployment of a resource.
These operations comprise tasks such as scaling up or down a VM based on the load, stopping a VM during off-peak hours, or migrating a VM to a different region to optimize costs.
As we previously described, the mutating webhook configuration is set on both the CREATE and \textbf{UPDATE} operations.
A possible UPDATE operation trigger could be a Kubernetes Cronjob that attaches a label \textit{``greenops-optimization": "919166400"} to the VmTemplate resource at a specific time of the day.
This could be useful to trigger specific policies that are only applied during the day 2 operations.
In addition to that, the UPDATE operation could be useful for VMs that have already been scheduled in a distant future (due to their deadline being very far in the future) to obtain a new scheduling decision based on new conditions (e.g. more recent carbon intensity forecast).

\subsection{OPA advanced features}

It is deemed useful to mention some of the advanced features of OPA that were not employed in the first iteration of the system described in this thesis but could be potentially useful in future developments or in other contexts where OPA is used.
\newline
To ensure data integrity of policies (i.e., to prevent unauthorized modifications or MITM (Man in the Middle) attacks) and to provide a mechanism for verifying the authenticity of policies, OPA provides a \textbf{policy signing} feature \cite{opa_signing}.
This feature allow policy developer to digitally sign their policies bundles by adding a file named ``\textit{.signature.json}''.
\newline
OPA also provides a more efficient way to distribute policies using \textbf{Delta Bundles} \cite{opa_delta_bundles}.
Normally, when a policy bundle is downloaded, OPA will download the entire bundle, erase and overwrite the current policies and data with the new ones.
Delta Bundles are composed of a single ``patch.json'' file that contains a set of JSON Patch operations that can be applied to the current data to update it to the new version. Currently only data updates are supported (``data.json") and not policy updates (``policy.rego") \cite{opa_delta_bundles}.
This feature could be used in the context of the system described in this thesis to update in a more efficient way the contextual data used by the policies. 
For instance, in the event of a region being added or removed by a public cloud provider, a delta bundle could be used to add only the new region and update the existing latency matrix.

\section{MLOps infrastructure}
\label{sec:mlops_infrastructure}

MLOps is the abbreviation of ``\textbf{Machine Learning Operations}", and it broadly refers to a set of methods designed to improve workflow procedures and automate machine learning deployments. 
It enables the reliable and efficient management, maintenance and deployment of models at scale \cite{mlops_ubuntu}.
A MLOps infrastructure is not necessarily required for multi-cloud resource management, but it is believed that AI models will be utilized in the future more and more to get \textbf{scheduling and management decisions}, as evidenced by recent studies discussed in section \ref{sec:ai_based_resource_management}.
It is therefore deemed important to describe the MLOps infrastructure deployed in a Kubernetes environment and leveraged by the system described in this thesis.

\subsection{MLOps purpose}

In a way, MLOps implements DevOps principles, tools and practices into typical Machine Learning workflows.
Its main purpose is to effectively industrialize the machine learning models lifecycle, enabling faster model development, selection, and deployment to production compared to traditional manual approaches.
Some principles of MLOps can be summarized as follows \cite{mlops_ubuntu}:
\begin{itemize}[itemsep=0.2pt, topsep=1pt]
  \item[$\bullet$] \textbf{Automation}: automate the entire model lifecycle, from training to deployment.
  \item[$\bullet$] \textbf{Versioning}: track and version models (with related data and code).
  \item[$\bullet$] \textbf{Reproducibility}: ensure that various model versions can be reproduced at any time.
  \item[$\bullet$] \textbf{Monitoring}: monitor models in production to ensure they are performing as expected.
  \item[$\bullet$] \textbf{Scalability}: scale models to handle increased workloads in a seamless manner.
  \item[$\bullet$] \textbf{Collaboration}: enable collaboration between data scientists, data engineers, and operations teams.
\end{itemize}

The proposed system deploys an MLOps infrastructure to manage the forecasting models that predict the carbon intensity of the electricity grid in different world regions.
In particular, \textbf{MLflow} is used for model tracking, model selection, and model storage, while \textbf{KServe} is used for model deployment.

\subsection{MLOps general architecture}

Figure \ref{fig:mlops} illustrates the general architecture of the MLOps infrastructure deployed in the Kubernetes environment. 
The architecture consists of two main components, described in the previous background sections: MLflow and KServe.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.75\linewidth]{images/mlflow_vertical.png}
  \caption{MLOps Architecture}
  \label{fig:mlops}
\end{figure}

\subsection{MLflow deployment configuration}

In the context of our system, MLflow is deployed on a Kubernetes cluster with a loosely coupled architecture as can be seen in Figure \ref{fig:mlflow_config}, where the MLflow Tracking Server is decoupled from its storage: the metadata store (backend store) and the artifact store.
This configuration is the most common in production environments, as it allows for better scalability and environment flexibility.
The metadata store chosen for the system is \textbf{CrateDB}, while the artifact store is the \textbf{SeaweedFS} object storage service. \\

\begin{figure}[t]
  \centering
  \includegraphics[width=0.50\linewidth]{images/mlflow_config.png}
  \caption{MLflow deployment configuration}
  \label{fig:mlflow_config}
\end{figure}

During the design phase, \textbf{alternative configurations} were considered.
One of the alternative configurations was to use a single CrateDB instance as both the metadata store and the artifact store. This configuration was not implemented due to the lack of native support of object storage in CrateDB.
A second theorized approach was to use a sidecar container with the duty of watching the MLflow Tracking Server local directory in the container filesystem (e.g., using \textit{watchdog} Python package), packaging the model artifacts as an OCI image, and uploading them to an image registry. This approach could be implemented to offer an alternative to the MLflow Tracking Server artifact store in environments where adding a new storage service is not feasible.

\subsection{Model deployment}
\label{sec:model_deployment}

Our specific use case requires the deployment of multiple models, each corresponding to a different region.
This is true if we want to achieve better model performance compared to a single model that tries to predict the carbon intensity of all the regions.
The current strategy adopted for the system is the following: \textbf{one model per region} is deployed, and a \textbf{generic model is used as a fallback} if the specific model is not available.
This translates into the deployment of multiple InferenceServices, each corresponding to a specific region, and one InferenceService that acts as a fallback.
This setting introduces a quite large amount of overhead in terms of resources, since each InferenceService sets up a whole new set of resources (e.g., large Kubernetes pods with underlying model serving environments) for each model.
Figure \ref{fig:forecaster} illustrates the configuration of the InferenceServices used to deploy the forecasting models in the system.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.85\linewidth]{images/forecaster.png}
  \caption{Model deployment configuration and GreenOps system integration}
  \label{fig:forecaster}
\end{figure}

In particular, in our specific use case, since the forecasting models are PyTorch models, packaged by MLflow as described in the previous section, InferenceServices with ``kserve-mlserver" ClusterServingRuntime are used. 
As a matter of fact, this runtime is the one that supports models packaged with MLflow \cite{kserve_mlflow}.
Listing \ref{lst:inference_service} shows an example of the InferenceService Custom Resource used to deploy a model in the system, namely the forectaser related to the ``\textit{IT-NO}'' ElectricityMaps region (northern Italy) \\

\begin{lstlisting}[language=yaml, caption={InferenceService Custom Resource example}, label={lst:inference_service}]
apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: "forecaster-IT-NO"
spec:
  predictor:
    serviceAccountName: sa-s3creds
    model:
      modelFormat:
        name: mlflow
      protocolVersion: v2  
      storageUri: s3://mlartifacts/forecaster-IT-NO
\end{lstlisting}

We can see that the InferenceService CR showed in \ref{lst:inference_service} is quite simple and self-explanatory. 
The most important fields are the \textit{model} field, which specifies the model format, the protocol version, and the storage URI.
The storage URI is the location of the model artifacts in the artifact store, which in the case of the system is the SeaweedFS object storage service with S3 compatibility. 
At the location specified by the storage URI, the model artifacts are stored in the self-contained directory created by MLflow after the model training session as described in section \ref{sec:mlflow_tracking_server}.

\subsubsection{Model compatibility}

In scenarios where ML models produce outputs in formats not directly compatible with the serving runtime, some workarounds are necessary, such as model wrapping. This process involves adapting the model's input and output interfaces to align with the expected formats of the serving runtime, ensuring a correct model deployment.
For instance, in the case of the forecasting models, the output format was not directly compatible with the serving runtime: the output was a custom defined class specific to the model, instead of a single tensor. 
Therefore, a simple model wrapping was needed to make the model compatible with the serving runtime, effectively extracting a subset of the model output fields and returning them as a single tensor.

\section{Impact framework potential integration}
\label{sec:impact_framework_integration}

A potential integration with the Green Software Foundation's Impact Framework is envisioned for the system.
Currently, said integration is not implemented, but it is deemed a valuable addition to the system in future iterations.
As briefly described in section \ref{sec:impact_framework}, the Impact Framework is a tool that allows the creation of pipelines for data extraction, transformation, and calculations.
In addition, Impact Framework provides a database of cloud instances (a CSV file) with their respective specifications (e.g., CPU model, CPU TDP, GPU model, GPU TDP, memory, etc), which can be used to estimate the power consumption of a cloud instance.
Listing \ref{lst:cloud_metadata_example} shows an example of a simple Impact Framework pipeline that extracts the metadata of a cloud instance.
As a matter of fact the pipeline is composed of just one step that extract the metadata of the Azure \textit{Standard\_A1\_v2} instance. \newline

\begin{lstlisting}[language=yaml, caption={Cloud Metadata extraction example}, label={lst:cloud_metadata_example}]
name: cloud-instance-metadata-extraction
description: null
tags: null
initialize:
  plugins:
    cloud-instance-metadata:
      path: builtin
      method: CSVLookup
      config:
        filepath: >-
          https://raw.githubusercontent.com/Green-Software-Foundation/if-data/main/cloud-metdata-azure-instances.csv
        query:
          instance-class: cloud/instance-type
        output: '*'
...
tree:
children:
  child:
    pipeline:
      compute:
        - cloud-instance-metadata
    @\yellowhl{inputs:}@
      - timestamp: 2023-08-06T00:00
        @\yellowhl{cloud/provider: azure}@
        @\yellowhl{cloud/instance-type: Standard\_A1\_v2}@
    @\yellowhl{outputs:}@
      - timestamp: 2023-08-06T00:00
        @\yellowhl{cloud/provider: azure}@
        @\yellowhl{cloud/instance-type: Standard\_A1\_v2}@
        cpu-cores-available: 52
        cpu-cores-utilized: 1
        cpu-manufacturer: Intel
        cpu-model-name: >-
          Intel® Xeon® Platinum 8272CL,Intel® Xeon® 8171M 2.1 GHz,Intel® Xeon®
          E5-2673 v4 2.3 GHz,Intel® Xeon® E5-2673 v3 2.4 GHz
        @\yellowhl{cpu-tdp: 205}@
        gpu-count: nan
        gpu-model-name: nan
        gpu-tdp: nan
        memory-available: 2
\end{lstlisting}

As we can see from the example, the pipeline is composed of two main sections: the \textit{initialize} section, where the plugins (pipelines steps) are defined, and the \textit{tree} section, where the actual pipeline is defined.
Among the output fields of the pipeline, we can see the \textit{cpu-tdp} field, which represents the \textbf{\textit{Thermal Design Power}} of the CPU of the Azure Standard\_A1\_v2 instance.
This field could be used in a more complex pipeline to estimate the power consumption of the instance, and therefore its carbon footprint.
An example of such pipeline is described in the Impact Framework documentation \cite{impact_framework_pipeline} and is shown in Figure \ref{fig:impact_framework}.
A series of steps are performed to estimate the carbon footprint of a cloud instance, starting from the metadata extraction, to the power consumption estimation (with conversions), to the final carbon footprint calculation.

\newpage

\begin{figure}[H]
  \centering
  \includegraphics[width=1\linewidth]{images/impact_framework.png}
  \caption{Impact Framework pipeline to estimate carbon footprint of a cloud instance}
  \label{fig:impact_framework}
\end{figure}


\newpage

\section{End-to-End workflow}
\label{sec:e2eflow}

In this section we will provide a complete end-to-end workflow of the system, from the user request to the final provision of the workload.
First, we will describe the workflow in a high-level manner, and then we will provide a detailed sequence diagram (Figure \ref{fig:sequence_diagram}) that illustrates the interactions between the various components of the system.
We must note that the workflow described in this section is a simplified version of the actual system, as it does not include some low level details.

\begin{enumerate}
  \item User (Developer, Data Scientist, etc) selects the VmTemplate Composition card on the Krateo Composable Portal (web-based user interface)
  \item User fills in the VmTemplate Composition form with the required fields (e.g., MinCPU, MinRAM, Deadline, Duration, MaxLatency)
  \item This triggers and Helm install of the VmTemplate Composition on the cluster
  \item A VmTemplate Composition CREATE API request is sent to the Kubernetes API server
  \item Authentication and Authorization checks are performed by the Kubernetes API server
  \item The VmTemplate Composition CREATE API request is intercepted by a Kubernetes mutating webhook configured with OPA as webhook server
  \item AdmissionReview request is sent to the OPA server
  \item OPA evaluates the AdmissionReview request against the policies
  \begin{enumerate}
    \item Cloud provider is selected among the available providers
    \item Eligible regions are calculated based on the selected cloud provider and the MaxLatency parameter
    \item GDPR policy (if enabled) is evaluated and a subset of eligible regions is returned
    \item A scheduling request is sent to the GreenOps Scheduler along with the eligible regions and a set of parameters
    \item The GreenOps Scheduler returns a decision with the selected region and scheduling time
    \item OPA maps the return ElectricityMaps region name to the Cloud Provider region name
    \item OPA creates the JSON patch with the provider, schedulingRegion, and schedulingTime fields
    \item OPA crafts and sends the AdmissionReview response to the Kubernetes API server 
  \end{enumerate}
  \item The Kubernetes mutating webhook receives the AdmissionReview response and mutates the VmTemplate Composition specification
  \item Kubernetes API server perform resource validation
  \item The VmTemplate Composition is persisted in the etcd database
  \item A periodic Helm upgrade operation is done by Krateo composition-dynamic-controller
  \item After schedulingTime is reached, an Helm upgrade operation will install the provider-specific manifests for VM provisioning on the Kubernetes Cluster
  \item Cloud provider operator (e.g., Azure Service Operator) that is constantly watching for new provider-specific resources is triggered
  \item The cloud provider operator provisions the VM (and required resources) on the cloud provider
  \item The VM is up and running on the cloud
\end{enumerate}

\begin{sidewaysfigure}
  \centering
  \includegraphics[width=\textheight]{images/sequence_diagram.png}
  \caption{Sequence diagram of the end-to-end workflow}
  \label{fig:sequence_diagram}
\end{sidewaysfigure}

\newpage
